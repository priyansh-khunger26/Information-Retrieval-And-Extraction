{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Information Retrieval and Extraction (IRE) - 2024**\n",
    "### **Assignment 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Goal\n",
    "\n",
    "This assignment will introduce you to two fundamental information retrieval models: the **boolean model** and the **vector space model**. You'll learn how to identify important keywords in documents and use them to determine which documents are most relevant to a search query.\n",
    "\n",
    "We'll start with the simple boolean model, then explore the more advanced vector space model. You'll learn how to apply **TF-IDF** and **BM25**- two methods for weighing the importance of terms- to represent documents and queries as vectors. Finally, you'll use these vectors to rank documents by their relevance to a given query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction\n",
    "\n",
    "### 2.1 Stemming\n",
    "<font size=\"3\">\n",
    "    \n",
    "A <b><i>stem</b></i> is the part of a word to which affixes (suffixes and prefixes) can be attached to form new\n",
    "words. A few examples of words that have the same stem are given below.\n",
    "- 'house', 'houses', 'housing'\n",
    "- 'set', 'sets', 'reset', 'setting', 'setters'\n",
    "- 'tall', 'taller', 'tallest'\n",
    "- 'steam', 'steamers', 'steamier', 'steaming'\n",
    "    \n",
    "<b><i>Stemming</b></i> is the process of extracting for each given word its corresponding stem. Several popular implementations for the English language can be found <a href=\"http://tartarus.org/~martin/PorterStemmer\" target=\"_blank\">here</a>.\n",
    "    \n",
    "</font>\n",
    "\n",
    "### 2.2 Stop Words\n",
    "<font size=\"3\">\n",
    "    \n",
    "<b><i>Stop words</b></i> are those words which are judged to be so common that they have very little information or discrimination power. Such words are usually useless for information retrieval. Thus, in order to enhance system performance, stop words can be left out of consideration. In online systems they are not indexed, and therefore are not searchable. A list of English <b><i>stop words</b></i> is given in the provided file <b>english.stop</b>.\n",
    "    \n",
    "</font>\n",
    "\n",
    "### 2.3 Term Frequency-Inverse Documents Frequency\n",
    "<font size=\"3\">\n",
    "\n",
    "The <b><i>tf-idf</b></i> is a weight used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is for a document in a collection of documents or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word\n",
    "in the corpus. Variations of the <b><i>tf-idf</b></i>  weighting scheme are often used by search engines as a central tool in scoring and ranking a documents relevance given a user query.\n",
    "The <b><i>term frequency</b></i> $tf_{ij}$ is the weight of a term $t_{i}$ in a document $d_{j}$ computed according to\n",
    "\\begin{equation}\n",
    "tf_{ij}= \\frac{freq_{ij}}{\\max\\limits_{k} freq_{kj}},\n",
    "\\end{equation}\n",
    "where $freq_{ij}$ is the number of occurrences of the term $t_{i}$ in the document $d_{j}$. As a result, the most frequent term $t_{\\bar{i}}$\n",
    "in a document $d_{j}$ always has $tf_{\\bar{i}j} = 1$.\n",
    "    \n",
    "The <b><i>inverse document frequency</b></i> $idf_{i}$ is a measure of the general importance of the term $t_{i}$ in\n",
    "the whole corpus. It is obtained by dividing the number of all documents by the number of\n",
    "documents containing the term  $t_{i}$, and then taking the logarithm of that quotient as\n",
    "\\begin{equation}\n",
    "idf_{i}= \\log \\frac{N}{n_{i}},\n",
    "\\end{equation}\n",
    "where $N$ is the total number of documents, and $n_{i}$ is the number of documents in which the\n",
    "term $t_{i}$ appears.\n",
    "Using the <b><i>term frequency</b></i> $tf_{ij}$ and the <b><i>inverse document frequency</b></i>  $idf_{i}$ as described above, we obtain the most used term-weighting scheme in text\n",
    "retrieval defned as\n",
    "\\begin{equation}\n",
    "w_{ij}= tf_{ij} \\cdot idf_{i}.\n",
    "\\end{equation}\n",
    "A high weight $w_{ij}$ is reached by a high frequency of the term $t_{i}$ in the document $d_{j}$ and a low frequency of the term $t_{i}$ in the whole collection of documents. Hence, the weights will tend to filter out common terms that appear in many documents in the collection.\n",
    "\n",
    "</font>\n",
    "\n",
    "### 2.4 BM25\n",
    "<font size=\"3\">\n",
    "\n",
    "**BM25**, also known as Okapi BM25, is a ranking function used by search engines to estimate the relevance of documents to a given search query. It improves upon the classic TF-IDF model by incorporating document length normalization and tunable parameters. \n",
    "\n",
    "**Formulation:**\n",
    "\n",
    "The BM25 score for a document *D* and query *Q* is calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "Score(D, Q) = \\sum_{i=1}^{n} IDF(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{avgdl})}\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "*   **$q_1$, ..., $q_n$** are the query terms.\n",
    "*   **$f(q_i, D)$** is the term frequency of $q_i$ in document *D*.\n",
    "*   **$|D|$** is the length of document *D*.\n",
    "*   **$avgdl$** is the average document length in the collection.\n",
    "*   **$k_1$** and **$b$** are tunable parameters:\n",
    "    *   **$k_1$** controls the impact of term frequency.\n",
    "    *   **$b$** controls the impact of document length.\n",
    "\n",
    "**IDF(qi)** is calculated similarly to TF-IDF:\n",
    "\n",
    "\\begin{equation}\n",
    "IDF(q_i) = log \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "*   **$N$** is the total number of documents in the corpus.\n",
    "*   **$n(q_i)$** is the number of documents containing $q_i$.\n",
    "\n",
    "</font> \n",
    "\n",
    "### 2.5 Boolean and Vector model\n",
    "\n",
    "<font size=\"3\">\n",
    "\n",
    "The <b><i>boolean model</b></i> is a simple retrieval model based set theory on <b><i>boolean algebra</b></i>. The concept of a set is intuitive, <b><i>index terms</b></i> and queries are specified as <b><i>boolean</b></i> expressions. The <b><i>boolean model</b></i> provides easy tools that are easy to be grasped by a common user of an information retrieval system. \n",
    "The <b><i>vector model</b></i> represents documents and queries as vectors. The dimensionality of each vector is fixed, so that we can compare similarity of each document $d_{j}$ and each query $q_{l}$ based on the <b><i>cosine of the angle</b></i> between two vectors that represent them. It is expressed according to following rules\n",
    "\\begin{equation}\n",
    "sim(d_{j},q_{l})=\\frac{\\langle\\vec{d_{j}},\\vec{q_{l}}\\rangle}{||\\vec{d_{j}}||\\cdot||\\vec{q_{l}}||}.\n",
    "\\end{equation}\n",
    "\n",
    "Each dimension corresponds to a unique term chosen in advance. Its value describes the significance of the term with respect the document. In the simplest case, it can be the number of times the term occurs in the document. The <b><i>vector model</b></i> disregards the order in which the terms appear in a document.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. List of Tasks\n",
    "\n",
    "Implement the below tasks **in sequence**. For any task that requires additional explanation, include it in your report.\n",
    "\n",
    "1. **Data Preprocessing and Tokenization:** `(6 marks)`\n",
    "    *   Read all documents from the provided folder.\n",
    "    *   Clean the data by removing punctuations, special characters, and digits.\n",
    "    *   If you choose to perform any additional data cleaning steps, clearly document these in your report.\n",
    "    *   Apply tokenization to break down the text into individual words (or tokens).\n",
    "    *   Perform stemming on the tokenized words using the Porter Stemming Algorithm for each document.\n",
    "\n",
    "3. **Term Weighting and Retrieval Model Construction:** \n",
    "    *   **Compute TF-IDF:** Calculate the TF-IDF weight for each term in each document using the formulas provided in section 2.3. `(4 marks)`\n",
    "    *   **Compute BM25:** Calculate the BM25 score for each document given a set of queries using the formula provided in section 2.4. Use a suitable value of $k_1$ and $b$. `(4 marks)`\n",
    "    *   **Implementation of Retrieval Models:**\n",
    "        *   **Boolean Model:** `(4 marks)`\n",
    "            *   Determine the top *p* most frequent stems across the entire corpus. You are encouraged to experiment with different values of *p* to observe their effects.\n",
    "            *   Represent each document as a binary vector. Each element in the vector corresponds to a specific top-*p* stem, and its value (1 or 0) indicates the presence or absence of that stem in the document.\n",
    "            *   For each query provided:\n",
    "                *   Represent the query as a binary vector in the same manner as the documents.\n",
    "                *   Retrieve documents where the logical AND operation between the document vector and the query vector results in a vector containing all '1's, signifying that all terms in the query are present in the retrieved document. \n",
    "        *   **Vector Model (TF-IDF and BM25):** `(6+6+10 = 22 marks)`\n",
    "            *   **TF-IDF:** Use the calculated TF-IDF matrix to construct a vector space model. Represent each document as a vector within this TF-IDF space. \n",
    "            *   **BM25:** Additionally, create a vector space model based on BM25 scores. Represent each document as a vector where each dimension corresponds to a query, and the value is the document's BM25 score for that query.\n",
    "            *   For each provided query: \n",
    "                *   Calculate the query's representation in both the TF-IDF space and the BM25 space.\n",
    "                *   Retrieve documents based on the cosine similarity between the query vector and the document vectors in both spaces. This means you'll be retrieving documents using two variations of the Vector Model: one based on TF-IDF and the other on BM25.\n",
    "\n",
    "4. **Stop Word Removal and Frequency Distribution:** `(2 marks)`\n",
    "    *   Load the stop words from the `english.stop` file.\n",
    "    *   Remove the stop words from both the documents and the provided queries.\n",
    "    *   Repeat step 2 (stemming and frequency distribution) on the data without stop words. Compare and discuss the differences in the frequency distributions before and after removing stop words.\n",
    "\n",
    "5. **Retrieval with Stop Word Removal:** `(34 marks)`\n",
    "    *   Repeat step 3 (term weighting, model construction, and retrieval) using the data with stop words removed.\n",
    "    *   Compare the retrieval results (retrieved documents for the same queries) before and after removing stop words. Analyze and discuss the impact of stop word removal on the retrieval performance of each model.\n",
    "\n",
    "6. **Document Clustering:** `(10 marks)`\n",
    "    *   Using both the TF-IDF matrix and the BM25 scores as document representations, apply a clustering algorithm of your choice from scikit-learn (e.g., K-Means, Agglomerative Clustering) to group similar documents together.\n",
    "    *   Visualize the clustering results (e.g., using scatter plots or dendrograms) and analyze the clusters formed. Discuss any interesting findings or observations about the document groupings.\n",
    "\n",
    "7. **Conclusions:** `(14 marks)`\n",
    "    *   Summarize the key findings of your experiments and analysis.\n",
    "    *   Discuss the strengths and weaknesses of each retrieval model (Boolean, Vector, TF-IDF, BM25) based on your observations.\n",
    "    *   Comment on the impact of stemming and stop word removal on the retrieval performance.\n",
    "    *   Provide any insights gained from the document clustering step. \n",
    "\n",
    "Remember to include clear explanations, code implementations, visualizations, and insightful analysis in your report to support your findings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "## 4. DATASET: Documents & Queries\n",
    "\n",
    "The documents dataset for this assignment can be downloaded from: [Documents Dataset](https://drive.google.com/file/d/11Prvjiswc16CwYClWSIDoWzmsuTaCp4O/view?usp=sharing).\n",
    "\n",
    "There are 10 separate queries allotted to each of you, and they can be found in the file: [A1 Queries](https://docs.google.com/spreadsheets/d/1NcVRXQKr5kCexyYXrJEXaEcHPngFqruI9Im-BTiOVD0/edit?usp=sharing).\n",
    "\n",
    "> Note that you are expected to show the analyses of _your_ findings on the queries assigned to _you_.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Assessment\n",
    "\n",
    "<font size=\"3\">\n",
    "\n",
    "The assessment is based on your **code and report**. \n",
    "\n",
    "**Code:**\n",
    "\n",
    "*   Submit your code as a **single, well-structured Jupyter Notebook (`.ipynb`) file**. \n",
    "*   **All cells in the notebook must be executed to show the desired outputs**.\n",
    "*   While you may use functions defined in separate `.py` files for better organization, **ensure all functions are called and executed within the notebook to demonstrate their functionality and results.**\n",
    "*   Organize your code clearly with comments to guide the reader through your implementation.\n",
    "\n",
    "> *Expectations* : You will be, at the very least, expected to give a brief code walkthrough, and explain the overall organization of the same.\n",
    "\n",
    "**Report:**\n",
    "(_Different from the Notebook_)\n",
    "*   Submit your report as a PDF document **(`Report.pdf`)**.\n",
    "\n",
    "*   **For each model (TF-IDF, BM25, Boolean Model, Vector Model):**\n",
    "    *   Provide a concise description of the model.\n",
    "    *   Include any observations, analysis, or insights related to the model's performance. This could include:\n",
    "        *   Strengths and weaknesses of the model observed during experimentation.\n",
    "        *   Anything significant or noteworthy about the implementation or results.\n",
    "*   Use clear language and visual aids (tables, graphs) to enhance the readability and understanding of your report.\n",
    "\n",
    "> *Expectations* : The key point of difference between the Report and the Notebook would be the ablations you perform and presentability. You are expected to form and showcase an understanding of the various techniques that you encounter. For example, what is the effect of changing `b` in the BM25 metric, the coverage of such things in the report holds significant value for this part. Make sure to back up any reasoning you give with plots/equations wherever applicable.\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "*   Compress both the code and the report into a single zip file named **&lt;RollNumber&gt;\\_Assignment1.zip**, for example, if your roll number is 2021101099, your zip submission should be **2021101099_Assignment1.zip**. You do not need to submit the corpus to the portal.\n",
    "*   Submit the zip file on Moodle.\n",
    "\n",
    "**Evaluation:**\n",
    "\n",
    "*   Your submission will be evaluated based on the code's functionality, organization, and clarity, as well as the completeness, clarity, and insightful analysis presented in the report.\n",
    "*   An in-person evaluation will be conducted, where you will walk the evaluators through your code and report. Be prepared to answer questions and explain your implementation choices and findings.\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "*   The deadline for submission is **11th September 2024**. \n",
    "*   **No extensions will be granted.**\n",
    "\n",
    "</font>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
